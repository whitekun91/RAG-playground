# ğŸ§  RAG-playground

A **FastAPI-based Retrieval-Augmented Generation (RAG)** template for hybrid local and cloud LLM inference.  
This project automatically downloads Hugging Face embedding models, builds Chroma vector stores, and performs local inference via **vLLM**.  
> ğŸ”€ **Engine switch is manual**: choose between **local (vLLM)** and **openai** per request with a simple parameter (no automatic fallback).

---

## ğŸš€ Features

âœ… **FastAPI backend** for RAG  
âœ… **vLLM local inference** (e.g., Gemma, Mistral, Llama)  
âœ… **Manual engine switch** â€” `engine: "local" | "openai"` per request  
âœ… **Chroma vector database** for retrieval  
âœ… **CrossEncoder reranking** for better context  
âœ… **LangChain orchestration**  
âœ… Clean, modular codebase ready for on-prem or hybrid

---

## ğŸ—ï¸ System Architecture

```
ğŸ“„ Documents (.pdf, .docx, .pptx, .xlsx)
       â†“ (text/image extraction)
ğŸ§© LangChain + Chroma VectorDB
       â†“ (similarity search + reranking)
ğŸ§  Inference Engine (manual): vLLM (local)  |  OpenAI API (cloud)
       â†“
ğŸ¯ Answer with Evidence
```

---

## ğŸ§° Tech Stack

| Component | Description |
|------------|-------------|
| **Backend** | FastAPI |
| **Vector DB** | Chroma |
| **Embeddings** | Hugging Face (e.g. `jhgan/ko-sbert-sts`) |
| **Reranker** | `bge-reranker-v2-m3` |
| **Local LLM** | vLLM (e.g. `gemma-3-12b-it`) |
| **Cloud LLM** | OpenAI GPT family |
| **Orchestration** | LangChain |
| **Hardware** | Multi-GPU (CUDA_VISIBLE_DEVICES=0,1) supported |

---

## ğŸ“¦ Installation

### 1) Clone
```bash
git clone https://github.com/whitekun91/RAG-playground.git
cd RAG-playground
```

### 2) Virtual Env
```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate      # Windows
```

### 3) Install
```bash
pip install -U pip
pip install -r requirements.txt
```

> âš ï¸ Ensure your `torch`/`transformers` CUDA wheels match your system (e.g., cu121/cu126).

---

## âš™ï¸ Environment (`.env`)

```bash
# Embeddings & Reranker
EMB_MODEL_PATH=../models/embeddings/ko-sbert-sts
RERANKER_PATH=../models/reranker/bge-reranker-v2-m3

# Local LLM (vLLM OpenAI-compatible server)
VLLM_MODEL=gemma-3-12b-it
VLLM_URL=http://0.0.0.0:8000/v1/completions

# OpenAI (cloud)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-5
```
---

## â–¶ï¸ Run

### Start vLLM (Local)
```bash
CUDA_VISIBLE_DEVICES=0,1 
python -m vllm.entrypoints.openai.api_server \
    --model "./models/LM_Models/gemma-3-12b-it" \
    --served-model-name "gemma-3-12b-it" \
    --tokenizer "./models/LM_Models/gemma-3-12b-it" \
    --dtype bfloat16 \
    --max-model-len 8192 \
    --chat-template-content-format auto \
    --gpu-memory-utilization 0.65 \
    --tensor-parallel-size 2 \
    --max-num-seqs 16 \
    --swap-space 8 \
    --enable-log-requests \
    --port 8000

```

### Start FastAPI
```bash
uvicorn main:app --host 0.0.0.0 --port 5001 --reload
```

---

## ğŸ’¬ API â€” Manual Engine Switch

### Endpoint
`POST /ask`

### Request Body (common)
```json
{
  "question": "~~~~~~~",
  "engine": "local",
  "model": "gemma-3-12b-it",
  "top_p": 0.9,
  "temperature": 0.3,
  "max_tokens": 512
}
```

- `engine`: `"local"` | `"openai"` (required)
- `model`: For local, use the vLLM model name; for openai, use the OpenAI model name (if not entered, the default value from `.env` is used)
- Other sample parameters are optional

### cURL â€” Local (vLLM)
```bash
curl -X POST http://localhost:5001/ask   -H "Content-Type: application/json"   -d '{
    "question": "~~~~~~~~~~~",
    "engine": "local",
    "model": "gemma-3-12b-it",
    "temperature": 0.2,
    "top_p": 0.9,
    "max_tokens": 512
  }'
```

### cURL â€” OpenAI
```bash
curl -X POST http://localhost:5001/ask   -H "Content-Type: application/json"   -d '{
    "question": "~~~~~~~~~",
    "engine": "openai",
    "model": "gpt-5",
    "temperature": 0.2,
    "top_p": 0.9,
    "max_tokens": 512
  }'
```

### Response (example)
```json
{
  "answer": "~~~~~~~~~~",
  "evidence": [
    "documents/~~~~~~~~~~~~~.pdf"
  ],
  "engine": "local",
  "model_used": "gemma-3-12b-it"
}
```
---

## ğŸ§© Directory

```
RAG-playground/
â”‚
â”œâ”€â”€ app.py                           # FastAPI entry
â”œâ”€â”€ settings.py                      # Paths & env
â”‚
â”œâ”€â”€ components/                      # Core components (image, vectorDB, split)
â”‚   â”œâ”€â”€ image_load.py                # Image selector
â”‚   â”œâ”€â”€ select_vectordb.py           # VectorDB select
â”‚   â”œâ”€â”€ split_korean.py              # Korean text splitter
â”‚
â”œâ”€â”€ interface/                       # LLM/RAG chain interface
â”‚   â”œâ”€â”€ create_chain.py              # LLM/RAG chains
â”‚   â”œâ”€â”€ load_vector.py               # Chroma loaders (multi-DB)
â”‚   â”œâ”€â”€ rag_reranker.py              # RAG Reranker
â”‚
â”œâ”€â”€ documents/                       # Source files
â”‚   â”œâ”€â”€ vector_db/                   # Chroma (PDF)
â”‚   â”œâ”€â”€ create_vectordb_by_pdf.py    # Build Chroma from unstructured data (PDF/image extraction)
â”‚   â”œâ”€â”€ extracted_images/            # Extracted images from PDFs
â”‚   â”œâ”€â”€ raw/                         # Raw document files
â”‚
â”œâ”€â”€ models/                          # Model files
â”‚   â”œâ”€â”€ embeddings/                  # Embedding models
â”‚   â”œâ”€â”€ LM_Models/                   # Local LLM models
â”‚   â”œâ”€â”€ STT_Models/                  # Speech-to-text models
â”‚   â”œâ”€â”€ TTS_Models/                  # Text-to-speech models
â”‚   â”œâ”€â”€ llm.py                       # LLM logic
â”‚   â”œâ”€â”€ stt.py                       # STT logic
â”‚   â”œâ”€â”€ tts.py                       # TTS logic
â”‚
â”œâ”€â”€ prompts/                         # Prompt templates
â”‚   â”œâ”€â”€ db_select_prompt.py          # VectorDB select prompt
â”‚   â”œâ”€â”€ image_detect_prompt.py       # Image call prompt
â”‚   â”œâ”€â”€ origin_prompt.py             # Base prompt
â”‚
â”œâ”€â”€ static/                          # Static files (frontend)
â”‚   â”œâ”€â”€ index.html                   # Main HTML page
â”‚   â”œâ”€â”€ style.css                    # CSS styles
â”‚
â”œâ”€â”€ outputs/                         # Output results
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ§ª Notes

- Ubuntu 22.04 / CUDA 12.x / Python 3.10  
- vLLM 0.6+ / LangChain 0.2+  
- Multi-GPU and long-context friendly  
- Designed for **RAG** with verifiable evidence

---

## ğŸ“˜ Roadmap


- [ ] OpenAI Whisper STT + Bark TTS (integrated voice input/output)
- [ ] Smart document routing (automatic classification of PDF / DOCX / PPTX)
- [ ] Hybrid Engine Enhancement  
  â†’ Support hybrid RAG with selectable inference engines (Local vLLM / OpenAI API)  
  â†’ Add simple â€œtoggleâ€ parameter or UI for engine switching  
  â†’ Compare OpenAI model quality & latency for hybrid benchmarking
- [ ] Add multi-turn memory module (context retention based on conversation)
- [ ] Add retrieval-evidence visualization on frontend (document evidence visualization)

---

## ğŸ§‘â€ğŸ’» Author

**Minwoo Baek  **  
Senior AI Engineer | PhD Candidate (Big Data Applications)  
ğŸ’¼ Hanwha Momentum / AI Manufacturing R&D  
ğŸ“§ minwoo713@gmail.com  
ğŸ”— LinkedIn: https://www.linkedin.com/in/bjh713/

---

## ğŸ“œ License

MIT
